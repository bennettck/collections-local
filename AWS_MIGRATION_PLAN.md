# AWS Cloud Migration Plan - Collections Local

## Executive Summary

Migrate the collections-local image analysis application from local development to AWS cloud using provider-native serverless services with user authentication and multi-tenancy.

### Target Architecture
- **Auth**: AWS Cognito (user management, JWT tokens)
- **API**: Lambda + API Gateway (FastAPI via Mangum)
- **Database**: RDS PostgreSQL (public, no VPC) with JSONB + pgvector
- **Storage**: S3 (images with per-user isolation)
- **Workflow**: Event-driven Lambda functions (EventBridge)
- **Secrets**: AWS Systems Manager Parameter Store (FREE)
- **Infrastructure**: AWS CDK (Python)

### Estimated Monthly Cost
- **RDS PostgreSQL** (db.t4g.micro, public): $15-20
- **Lambda**: $5-15 (first 1M requests free)
- **API Gateway** (HTTP API): $1-2
- **S3**: $0.50
- **CloudWatch Logs**: $1-2
- **Parameter Store**: FREE
- **Total**: ~$22-40/month (small team usage)

**Savings from simplified architecture:**
- No VPC/NAT Gateway: Saves $32/month
- No RDS Proxy needed: Saves $11/month
- No users table sync complexity

### Key Simplifications

This plan uses a **pragmatic, simplified approach** for a small-scale application:

1. **No VPC** - Public RDS with security groups (saves $32/month, faster Lambda cold starts)
2. **No RLS** - Manual `WHERE user_id = ?` filters instead of Row-Level Security (easier to debug)
3. **No Users Table** - Use Cognito `sub` directly (no sync complexity, simpler schema)
4. **Manual Multi-tenancy** - Extract user_id from JWT ‚Üí add to all queries (straightforward, explicit)

**Trade-off**: Slightly more code (manual filters) in exchange for:
- Lower cost (~$20-30/month vs ~$60-70/month)
- Simpler architecture (no VPC, no connection pooling complexity)
- Easier debugging (explicit WHERE clauses vs invisible RLS)
- Faster cold starts (Lambda not in VPC)

---

## What's New Since v2.0 (December 2025)

**Important**: This migration plan was originally created several days ago. Since then, significant new features have been implemented that impact the migration strategy:

### 1. Multi-Turn Agentic Chat with LangGraph ‚≠ê **MAJOR**

**What Changed:**
- Added full conversational AI with persistent memory
- Uses LangGraph `create_react_agent` with SQLite checkpoint persistence
- Session management with 4-hour TTL and cleanup
- Dual tools: collection search + Tavily web search

**Impact on Migration:**
- **NEW**: DynamoDB required for conversation checkpoints
- **NEW**: Lambda #5 for session cleanup (EventBridge cron)
- **Cost Impact**: +$1-5/month for DynamoDB
- **Files Added**: `chat/agentic_chat.py`, `chat/conversation_manager.py`

**Current Implementation:**
```
data/conversations.db (SQLite)
  ‚îî‚îÄ langgraph-checkpoint-sqlite (SqliteSaver)
      ‚îú‚îÄ Checkpoints (agent state)
      ‚îú‚îÄ Session tracking
      ‚îî‚îÄ TTL-based cleanup
```

**AWS Migration Target:**
```
DynamoDB Table: collections-chat-checkpoints
  ‚îî‚îÄ Thread ID: {user_id}#{session_id}
      ‚îú‚îÄ Automatic TTL (4 hours)
      ‚îú‚îÄ No manual cleanup needed
      ‚îî‚îÄ Multi-tenant isolation
```

### 2. Tavily Web Search Integration üåê

**What Changed:**
- Added Tavily API integration as second tool in chat agent
- Enables web search alongside collection search
- Domain filtering and search depth configuration

**Impact on Migration:**
- **NEW**: Add `TAVILY_API_KEY` to Parameter Store
- **NEW**: Add `tavily-python` to Lambda dependencies
- **Cost Impact**: Negligible (API usage only)
- **Files Modified**: `chat/agentic_chat.py` (lines 88-89, 140-198)

### 3. ChromaDB Vector Store (Replacing sqlite-vec) üîÑ **CRITICAL**

**What Changed:**
- Completely replaced sqlite-vec with ChromaDB
- File-based persistence in `data/chroma_prod/` and `data/chroma_golden/`
- Explicit cosine similarity configuration
- LangChain VoyageAI embeddings integration

**Impact on Migration:**
- **CRITICAL**: ChromaDB is **incompatible with Lambda** (requires persistent disk)
- **MUST**: Migrate ChromaDB ‚Üí pgvector in RDS PostgreSQL
- **MUST**: Create `retrieval/pgvector_manager.py` to replace `retrieval/chroma_manager.py`
- **Benefit**: pgvector is 2.4x faster than ChromaDB and free (uses existing RDS)

**Current Implementation:**
```
ChromaVectorStoreManager
  ‚îú‚îÄ Persistent Chroma (file-based)
  ‚îú‚îÄ VoyageAI embeddings (voyage-3.5-lite)
  ‚îú‚îÄ Cosine similarity metric
  ‚îî‚îÄ Dual collections (prod/golden)
```

**AWS Migration Target:**
```
PgVectorManager
  ‚îú‚îÄ PostgreSQL pgvector extension
  ‚îú‚îÄ Same VoyageAI embeddings
  ‚îú‚îÄ Cosine similarity (<-> operator)
  ‚îî‚îÄ User-filtered queries
```

### 4. Enhanced LangChain Retriever Architecture üîç

**What Changed:**
- Sophisticated hybrid retrieval with Reciprocal Rank Fusion (RRF)
- Three retriever types: BM25, Vector, Hybrid
- All extend `BaseRetriever` for LangSmith evaluation
- Category filtering and score thresholds

**Impact on Migration:**
- BM25: SQLite FTS5 ‚Üí PostgreSQL tsvector
- Vector: ChromaDB ‚Üí pgvector
- Hybrid: Same RRF logic, new backends
- **Files Modified**: `retrieval/langchain_retrievers.py`

**Configuration:**
- RRF weights: 30% BM25, 70% Vector
- RRF constant: c=15
- Fetch multiplier: 2x for better fusion

### Summary of Changes

| Feature | Status | Migration Impact | Cost Impact |
|---------|--------|------------------|-------------|
| Multi-turn chat | NEW | DynamoDB + Lambda #5 | +$1-5/month |
| Tavily web search | NEW | Parameter Store secret | ~$0/month |
| ChromaDB vectors | CHANGED | Must migrate to pgvector | $0 (RDS) |
| LangChain retrievers | UPDATED | Update for PostgreSQL | $0 |

**Total Additional Cost**: +$7-12/month (new total: $29-52/month vs original $22-40/month)

**Migration Complexity**:
- Original plan: **LOW-MEDIUM**
- Updated plan: **MEDIUM** (due to ChromaDB ‚Üí pgvector migration and DynamoDB checkpointer)

---

## Architecture Overview

### Lambda Function Division

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  1. API Lambda (FastAPI)                            ‚îÇ
‚îÇ     - All HTTP endpoints (GET/POST/DELETE)          ‚îÇ
‚îÇ     - Chat endpoints (POST /chat, GET history)      ‚îÇ
‚îÇ     - Search endpoints (agentic, hybrid, BM25)      ‚îÇ
‚îÇ     - Manual workflow triggers                      ‚îÇ
‚îÇ     - User authentication (JWT validation)          ‚îÇ
‚îÇ     - DynamoDB checkpoint access for chat           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  2. Image Processor Lambda                          ‚îÇ
‚îÇ     - Triggered by S3 upload event                  ‚îÇ
‚îÇ     - Resize images, create thumbnails              ‚îÇ
‚îÇ     - Publishes "ImageProcessed" event              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  3. Analysis Lambda                                  ‚îÇ
‚îÇ     - Triggered by EventBridge event                ‚îÇ
‚îÇ     - Calls Claude/GPT vision APIs                  ‚îÇ
‚îÇ     - Stores analysis in PostgreSQL                 ‚îÇ
‚îÇ     - Publishes "AnalysisComplete" event            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  4. Embedding Lambda                                 ‚îÇ
‚îÇ     - Triggered by EventBridge event                ‚îÇ
‚îÇ     - Calls VoyageAI for vector embeddings          ‚îÇ
‚îÇ     - Stores in PostgreSQL (pgvector)               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  5. Conversation Cleanup Lambda (NEW)               ‚îÇ
‚îÇ     - Triggered by EventBridge cron (hourly)        ‚îÇ
‚îÇ     - Monitors DynamoDB checkpoint expiration       ‚îÇ
‚îÇ     - Logs cleanup statistics                       ‚îÇ
‚îÇ     - Note: DynamoDB TTL handles actual deletion    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Automated Workflow

```
User uploads image (POST /items)
    ‚Üì
API Lambda ‚Üí S3.put_object()
    ‚Üì
S3 Event ‚Üí Image Processor Lambda
    ‚Üì
EventBridge: "ImageProcessed" ‚Üí Analysis Lambda
    ‚Üì
EventBridge: "AnalysisComplete" ‚Üí Embedding Lambda
    ‚Üì
Complete
```

### Manual Workflow (Independent Calls)

```
POST /items/{id}/analyze     ‚Üí Direct invoke Analysis Lambda
POST /items/{id}/resize-only ‚Üí Direct invoke Image Processor Lambda
POST /vector-index/rebuild   ‚Üí Batch invoke Embedding Lambda
```

---

## Database Schema Changes

### Multi-Tenancy with Manual Filtering (Simplified)

```sql
-- Items (with user_id from Cognito JWT)
CREATE TABLE items (
    id TEXT PRIMARY KEY,
    user_id TEXT NOT NULL,             -- Cognito sub from JWT token
    filename TEXT NOT NULL,
    original_filename TEXT,
    file_path TEXT NOT NULL,           -- s3://bucket/{user_id}/images/{uuid}.jpg
    thumbnail_path TEXT,               -- s3://bucket/{user_id}/thumbnails/{uuid}.jpg
    file_size INTEGER,
    mime_type TEXT,
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW()
);

CREATE INDEX idx_items_user_id ON items(user_id);

-- Analyses (JSONB instead of TEXT)
CREATE TABLE analyses (
    id TEXT PRIMARY KEY,
    item_id TEXT NOT NULL REFERENCES items(id) ON DELETE CASCADE,
    user_id TEXT NOT NULL,             -- Cognito sub from JWT token
    version INTEGER DEFAULT 1,
    category TEXT,
    summary TEXT,
    raw_response JSONB,                -- Changed from TEXT to JSONB
    provider_used TEXT,
    model_used TEXT,
    trace_id TEXT,
    created_at TIMESTAMP DEFAULT NOW()
);

CREATE INDEX idx_analyses_user_id ON analyses(user_id);

-- Embeddings with pgvector
CREATE EXTENSION IF NOT EXISTS vector;

CREATE TABLE embeddings (
    id TEXT PRIMARY KEY,
    item_id TEXT NOT NULL REFERENCES items(id) ON DELETE CASCADE,
    user_id TEXT NOT NULL,             -- Cognito sub from JWT token
    analysis_id TEXT NOT NULL REFERENCES analyses(id) ON DELETE CASCADE,
    embedding vector(1024),
    embedding_model TEXT NOT NULL,
    embedding_dimensions INTEGER NOT NULL,
    embedding_source JSONB,             -- Changed from TEXT to JSONB
    created_at TIMESTAMP DEFAULT NOW()
);

CREATE INDEX idx_embeddings_item_id ON embeddings(item_id);
CREATE INDEX idx_embeddings_user_id ON embeddings(user_id);

-- pgvector index for cosine similarity search (CRITICAL for performance)
CREATE INDEX idx_embeddings_vector ON embeddings
    USING ivfflat (embedding vector_cosine_ops)
    WITH (lists = 100);  -- Adjust lists based on data size (sqrt of row count)

-- PostgreSQL Full-Text Search (replaces SQLite FTS5)
ALTER TABLE analyses ADD COLUMN search_vector tsvector;

CREATE INDEX idx_analyses_search ON analyses USING GIN(search_vector);

-- Update function for search vector
CREATE OR REPLACE FUNCTION update_search_vector()
RETURNS TRIGGER AS $$
BEGIN
    NEW.search_vector :=
        setweight(to_tsvector('english', COALESCE(NEW.summary, '')), 'A') ||
        setweight(to_tsvector('english', COALESCE(NEW.category, '')), 'B');
    RETURN NEW;
END;
$$ LANGUAGE plpgsql;

CREATE TRIGGER analyses_search_update
    BEFORE INSERT OR UPDATE ON analyses
    FOR EACH ROW
    EXECUTE FUNCTION update_search_vector();
```

### DynamoDB Schema for Conversation Checkpoints (NEW)

**Table**: `collections-chat-checkpoints`

```
Primary Key: thread_id (String)
Sort Key: checkpoint_id (String)
TTL Attribute: expires_at (Number, epoch timestamp)

Table Attributes:
- thread_id: String (PK) - Format: "{user_id}#{session_id}"
- checkpoint_id: String (SK) - UUID for each checkpoint
- checkpoint: Binary - Serialized LangGraph agent state
- metadata: Map - Additional checkpoint metadata
- created_at: Number - Epoch timestamp
- expires_at: Number - Epoch timestamp (4 hours from creation)
- last_activity: Number - Epoch timestamp (updated on each interaction)
- user_id: String - Cognito user ID (extracted from thread_id)
- message_count: Number - Number of messages in conversation

Global Secondary Index: user_id-last_activity-index
- Partition Key: user_id (String)
- Sort Key: last_activity (Number)
- Projection: ALL
- Purpose: Query all sessions for a specific user

Table Settings:
- Billing Mode: ON_DEMAND (pay per request)
- TTL Enabled: YES (expires_at attribute)
- Point-in-Time Recovery: Recommended for production
- Encryption: AWS managed key (default)

Example Thread ID Format:
- User ID: "us-east-1:12345678-1234-1234-1234-123456789012"
- Session ID: "chat-2024-12-27-abc123"
- Thread ID: "us-east-1:12345678-1234-1234-1234-123456789012#chat-2024-12-27-abc123"

Cost Estimate (On-Demand):
- 1,000 messages/day = 1,000 writes + 2,000 reads
- Monthly: 30k writes ($0.04) + 60k reads ($0.015) + storage ($0.0003)
- Total: ~$0.06-5/month depending on usage
```

---

## Secrets Management

### AWS Systems Manager Parameter Store (FREE)

All secrets stored in Parameter Store as encrypted parameters:

```
/collections/anthropic-api-key      (SecureString)
/collections/openai-api-key         (SecureString)
/collections/voyage-api-key         (SecureString)
/collections/tavily-api-key         (SecureString) - NEW: For web search in chat
/collections/langsmith-api-key      (SecureString)
/collections/database-url           (SecureString) - Auto-generated by RDS
```

### Configuration (Lambda Environment Variables)

Non-sensitive configuration injected by CDK:

```
LANGSMITH_PROJECT=collections-local
LANGSMITH_ENDPOINT=https://api.smith.langchain.com
LANGSMITH_PROMPT_NAME=collections-app-initial
VOYAGE_EMBEDDING_MODEL=voyage-3.5-lite
BUCKET_NAME={auto-generated by CDK}
DB_SECRET_ARN={auto-generated by RDS}
```

---

## Conversation Persistence Strategy

### DynamoDB vs PostgreSQL Comparison

When migrating the conversation checkpoint system from SQLite, two main options exist:

| Aspect | DynamoDB (Recommended) | PostgreSQL in RDS |
|--------|----------------------|-------------------|
| **Performance** | Single-digit ms latency | Variable (connection overhead) |
| **Lambda Integration** | HTTP API (no connections) | TCP connections (pooling needed) |
| **Cold Start Impact** | Minimal | Significant without RDS Proxy |
| **TTL Cleanup** | Automatic (FREE) | Requires cleanup Lambda or cron |
| **Scaling** | Automatic, serverless | Manual (instance size) |
| **Cost (estimated)** | $1-5/month (on-demand) | $0 (existing RDS) BUT requires RDS Proxy ($11/month) for production |
| **Complexity** | Custom checkpointer needed | Can use `langgraph-checkpoint-postgres` |
| **Multi-tenancy** | Native (partition by user_id) | Row-level filtering |
| **Best For** | Serverless, variable load | Predictable load, existing RDS infrastructure |

### Chosen Approach: DynamoDB ‚úÖ

**Rationale:**
1. **Better Lambda performance**: No connection pooling overhead, HTTP-based access
2. **Lower total cost**: $1-5/month vs $11/month for RDS Proxy (required for production-grade connection management)
3. **Automatic TTL cleanup**: DynamoDB handles expiration natively (no cleanup Lambda needed for deletion)
4. **Serverless scaling**: Automatically handles traffic spikes
5. **Simpler architecture**: No VPC, no connection management, no failover complexity

**Implementation Details:**

**Thread ID Format for Multi-Tenancy:**
```python
# Extract user_id from JWT in middleware
user_id = request.state.user_id  # e.g., "us-east-1:12345678..."

# Prefix session_id with user_id for isolation
thread_id = f"{user_id}#{session_id}"
# Result: "us-east-1:12345678...#chat-2024-12-27-abc123"
```

**TTL Configuration:**
```python
import time
from datetime import timedelta

# Set expiration 4 hours from now
expires_at = int(time.time()) + int(timedelta(hours=4).total_seconds())

# DynamoDB automatically deletes items when TTL expires (no cost)
```

**GSI for User Queries:**
```python
# Query all sessions for a user
response = dynamodb.query(
    IndexName='user_id-last_activity-index',
    KeyConditionExpression='user_id = :uid',
    ExpressionAttributeValues={':uid': user_id},
    ScanIndexForward=False,  # Most recent first
    Limit=50
)
```

**Checkpointer Interface:**
```python
from langgraph.checkpoint.base import BaseCheckpointSaver

class DynamoDBCheckpointer(BaseCheckpointSaver):
    """LangGraph-compatible DynamoDB checkpointer."""

    def __init__(self, table_name: str, ttl_hours: int = 4):
        self.table = boto3.resource('dynamodb').Table(table_name)
        self.ttl_seconds = int(timedelta(hours=ttl_hours).total_seconds())

    def put(self, config, checkpoint, metadata):
        """Save checkpoint to DynamoDB with TTL."""
        thread_id = config['configurable']['thread_id']

        self.table.put_item(Item={
            'thread_id': thread_id,
            'checkpoint_id': checkpoint['id'],
            'checkpoint': self._serialize(checkpoint),
            'metadata': metadata,
            'created_at': int(time.time()),
            'expires_at': int(time.time()) + self.ttl_seconds,
            'user_id': thread_id.split('#')[0],  # Extract for GSI
        })

    def get(self, config):
        """Load checkpoint from DynamoDB."""
        # Implementation: query by thread_id, deserialize
        ...
```

**Cleanup Lambda (Monitoring Only):**
```python
# Note: Actual deletion is automatic via DynamoDB TTL
# This Lambda only logs statistics

def cleanup_handler(event, context):
    """Monitor expired checkpoints (DynamoDB TTL handles deletion)."""

    dynamodb = boto3.resource('dynamodb')
    table = dynamodb.Table(os.environ['CHECKPOINT_TABLE_NAME'])

    # Query recently expired items (for logging only)
    cutoff = int(time.time())

    # DynamoDB TTL typically deletes within 48 hours of expiration
    # This is acceptable for our 4-hour TTL use case

    logger.info(f"DynamoDB TTL cleanup in progress. No action needed.")
    return {"status": "automatic"}
```

---

## Vector Store Migration: ChromaDB ‚Üí pgvector

### Why ChromaDB Doesn't Work in Lambda

**Problem**: ChromaDB requires persistent file-based storage:
```python
# Current local implementation (incompatible with Lambda)
chroma_client = chromadb.PersistentClient(path="./data/chroma_prod")
```

**Lambda Limitations**:
- Ephemeral filesystem (only `/tmp` is writable, limited to 10GB)
- No persistent storage between invocations
- File-based databases don't survive cold starts
- Would require mounting EFS (adds $5-10/month + latency)

**Solution**: Migrate to pgvector in RDS PostgreSQL ‚úÖ

### Performance Comparison

| Metric | ChromaDB (Local) | pgvector (RDS) | Difference |
|--------|------------------|----------------|------------|
| **Search Latency** | 23.08ms | 9.81ms | **2.4x faster** |
| **Index Build Time** | ~45s (100 docs) | ~30s (100 docs) | 1.5x faster |
| **Memory Usage** | ~150MB | ~50MB (shared) | 3x less |
| **Storage** | 164KB (SQLite file) | Integrated in RDS | Consolidated |
| **Scaling** | File-based limits | Database-level | Better |
| **Cost** | N/A (local) | $0 (existing RDS) | FREE |

*Benchmark source: [ChromaDB vs pgvector comparison](https://github.com/Devparihar5/chromdb-vs-pgvector-benchmark)*

### Migration Workflow

**Step 1: Export from ChromaDB**
```python
# scripts/migrate/chroma_to_pgvector.py

from retrieval.chroma_manager import ChromaVectorStoreManager
import chromadb

# Load existing ChromaDB collection
chroma_manager = ChromaVectorStoreManager(
    database_path="./data/collections.db",
    persist_directory="./data/chroma_prod",
    collection_name="collections_vectors"
)

# Get all documents
collection = chroma_manager.chroma_client.get_collection("collections_vectors")
results = collection.get(include=['embeddings', 'documents', 'metadatas'])

# Results structure:
# {
#   'ids': ['item-uuid-1', 'item-uuid-2', ...],
#   'embeddings': [[0.1, 0.2, ...], [0.3, 0.4, ...], ...],
#   'documents': ['doc text 1', 'doc text 2', ...],
#   'metadatas': [{'item_id': '...', 'category': '...'}, ...]
# }
```

**Step 2: Transform for pgvector**
```python
# Prepare batch insert data
embeddings_data = []

for i, item_id in enumerate(results['ids']):
    embedding = results['embeddings'][i]
    metadata = results['metadatas'][i]

    # Extract user_id (add from test Cognito user for migration)
    user_id = os.environ['TEST_USER_ID']

    embeddings_data.append({
        'id': generate_uuid(),
        'item_id': item_id,
        'user_id': user_id,
        'analysis_id': metadata['analysis_id'],  # From metadata
        'embedding': embedding,  # pgvector handles list ‚Üí vector conversion
        'embedding_model': 'voyage-3.5-lite',
        'embedding_dimensions': len(embedding),
        'embedding_source': {'migrated_from': 'chromadb'},
    })
```

**Step 3: Batch Insert to PostgreSQL**
```python
import psycopg2
from psycopg2.extras import execute_batch

conn = psycopg2.connect(os.environ['DATABASE_URL'])
cursor = conn.cursor()

# Batch insert (efficient for large datasets)
insert_query = """
    INSERT INTO embeddings (
        id, item_id, user_id, analysis_id, embedding,
        embedding_model, embedding_dimensions, embedding_source
    ) VALUES (%s, %s, %s, %s, %s::vector, %s, %s, %s)
"""

execute_batch(
    cursor,
    insert_query,
    [(
        d['id'], d['item_id'], d['user_id'], d['analysis_id'],
        d['embedding'], d['embedding_model'], d['embedding_dimensions'],
        json.dumps(d['embedding_source'])
    ) for d in embeddings_data],
    page_size=100  # Batch size
)

conn.commit()
```

**Step 4: Validate Migration**
```python
# scripts/migrate/validate_vector_migration.py

# Compare counts
chroma_count = len(results['ids'])
pg_cursor.execute("SELECT COUNT(*) FROM embeddings WHERE user_id = %s", (user_id,))
pg_count = pg_cursor.fetchone()[0]

assert chroma_count == pg_count, f"Count mismatch: {chroma_count} vs {pg_count}"

# Test sample searches
sample_queries = ["modern furniture", "outdoor activities", "food photography"]

for query in sample_queries:
    # Generate query embedding
    query_embedding = generate_embedding(query)

    # Search pgvector
    pg_results = search_pgvector(query_embedding, k=10)

    # Search ChromaDB (for comparison)
    chroma_results = chroma_manager.similarity_search(query, k=10)

    # Compare top results (order may vary slightly due to distance calculation precision)
    pg_item_ids = {r['item_id'] for r in pg_results[:5]}
    chroma_item_ids = {r.metadata['item_id'] for r in chroma_results[:5]}

    overlap = len(pg_item_ids & chroma_item_ids)
    print(f"Query '{query}': {overlap}/5 overlap in top results")

    assert overlap >= 3, f"Low overlap for query '{query}': {overlap}/5"
```

### Cosine Similarity Configuration

**Critical**: Must use cosine distance (not L2) to match ChromaDB behavior:

```sql
-- Create index with cosine similarity operator
CREATE INDEX idx_embeddings_vector ON embeddings
    USING ivfflat (embedding vector_cosine_ops)  -- CRITICAL: cosine, not L2
    WITH (lists = 100);

-- Search query using cosine distance
SELECT item_id, embedding <-> %s AS distance  -- <-> is cosine distance
FROM embeddings
WHERE user_id = %s
ORDER BY embedding <-> %s  -- Ascending (smaller distance = more similar)
LIMIT 10;
```

**Distance Metrics Comparison**:
- `<->` Cosine distance (1 - cosine_similarity) - **USE THIS**
- `<#>` Negative inner product
- `<+>` L2 (Euclidean) distance - **DO NOT USE** (different results than ChromaDB)

### Code Changes Required

**Replace**: `retrieval/chroma_manager.py` (399 lines)
**With**: `retrieval/pgvector_manager.py` (new file, ~300 lines)

**Key Methods**:
```python
class PgVectorManager:
    """PostgreSQL + pgvector manager (replaces ChromaVectorStoreManager)."""

    def __init__(self, database_url: str, embedding_model: str):
        self.conn = psycopg2.connect(database_url)
        self.embedding_model = embedding_model
        # Use same VoyageAI embeddings
        self.embeddings = VoyageAIEmbeddings(model=embedding_model)

    def build_index(self, batch_size: int = 128) -> int:
        """Build pgvector index from analyses (same as ChromaDB)."""
        # Fetch items + analyses
        # Generate embeddings via VoyageAI
        # Batch insert into embeddings table
        ...

    def add_document(self, item_id: str, raw_response: dict, filename: str) -> bool:
        """Add/update single document (real-time sync)."""
        # Generate embedding
        # INSERT ... ON CONFLICT UPDATE
        ...

    def similarity_search(self, query: str, k: int = 10, user_id: str = None) -> List[dict]:
        """Cosine similarity search with user filtering."""
        query_embedding = self.embeddings.embed_query(query)

        cursor = self.conn.cursor()
        cursor.execute("""
            SELECT item_id, embedding <-> %s::vector AS distance, category, summary
            FROM embeddings
            WHERE user_id = %s
            ORDER BY embedding <-> %s::vector
            LIMIT %s
        """, (query_embedding, user_id, query_embedding, k))

        return cursor.fetchall()
```

**Update**: `retrieval/langchain_retrievers.py` (240 lines)
```python
# Before (ChromaDB)
class VectorLangChainRetriever(BaseRetriever):
    def __init__(self, chroma_manager: ChromaVectorStoreManager, ...):
        self.chroma_manager = chroma_manager
        self.vectorstore = chroma_manager.vectorstore

    def _get_relevant_documents(self, query: str) -> List[Document]:
        results = self.vectorstore.similarity_search(query, k=self.top_k)
        ...

# After (pgvector)
class VectorLangChainRetriever(BaseRetriever):
    def __init__(self, pgvector_manager: PgVectorManager, user_id: str, ...):
        self.pgvector_manager = pgvector_manager
        self.user_id = user_id

    def _get_relevant_documents(self, query: str) -> List[Document]:
        results = self.pgvector_manager.similarity_search(
            query, k=self.top_k, user_id=self.user_id
        )
        ...
```

### Validation & Rollback

**Validation Checklist**:
- [ ] Embedding counts match (ChromaDB vs pgvector)
- [ ] Top-10 results for test queries have ‚â•60% overlap
- [ ] Cosine distance metric confirmed (`<->` operator)
- [ ] User isolation works (different user_id results)
- [ ] Performance meets requirements (<500ms search latency)

**Rollback Plan**:
1. Keep `data/chroma_prod/` and `data/chroma_golden/` directories as backup
2. If issues found, revert `retrieval/` code changes
3. Use local ChromaDB for testing while fixing pgvector
4. Re-run migration script after fixes

---

## Code Migration Strategy

### How Much Code Changes?

The good news: **Most of your existing code works with minimal changes**. Here's the breakdown:

#### ‚úÖ **Copy/Paste Files (No changes needed)**

These files work as-is in Lambda:

1. **`llm.py`** - LLM analysis logic
   - Anthropic/OpenAI API calls ‚Üí unchanged
   - LangSmith integration ‚Üí unchanged
   - Just ensure environment variables come from Parameter Store instead of .env

2. **`embeddings.py`** - Vector embedding generation
   - VoyageAI API calls ‚Üí unchanged
   - Embedding logic ‚Üí unchanged

3. **`models.py`** - Pydantic models
   - Request/response schemas ‚Üí unchanged

4. **Most of `database.py`** - Database functions
   - SQL queries ‚Üí mostly unchanged (just swap SQLite ‚Üí PostgreSQL syntax)
   - Connection logic ‚Üí needs update for PostgreSQL

#### üîß **Files That Need Modifications**

**1. `main.py` ‚Üí API Lambda**

**Changes needed:**
```python
# ADD: Mangum adapter for Lambda
from mangum import Mangum

app = FastAPI(...)

# ADD: Lambda handler
handler = Mangum(app)

# ADD: Auth middleware
from middleware.auth import authenticate
app.middleware("http")(authenticate)

# CHANGE: All endpoints - add user_id filtering
# Before:
items = list_items(category=category, limit=limit, offset=offset)

# After:
items = list_items(
    user_id=request.state.user_id,  # NEW
    category=category,
    limit=limit,
    offset=offset
)

# CHANGE: File uploads - S3 instead of local filesystem
# Before:
async with aiofiles.open(file_path, "wb") as f:
    await f.write(content)

# After:
s3_client.put_object(
    Bucket=BUCKET_NAME,
    Key=f"{user_id}/images/{filename}",
    Body=content
)

# CHANGE: Image serving - pre-signed URLs
# Before:
return FileResponse(file_path)

# After:
url = s3_client.generate_presigned_url(
    'get_object',
    Params={'Bucket': BUCKET_NAME, 'Key': key},
    ExpiresIn=3600
)
return {"url": url}
```

**Estimate**: ~30-40 lines changed out of 981 lines (3-4% of file)

---

**2. `database.py` ‚Üí PostgreSQL Updates**

**Changes needed:**
```python
# CHANGE: Connection string
# Before:
conn = sqlite3.connect(db_path)

# After:
import psycopg2
conn = psycopg2.connect(os.environ['DATABASE_URL'])

# CHANGE: Query syntax (minor)
# Before (SQLite):
cursor.execute("INSERT ... RETURNING *")
row = cursor.fetchone()

# After (PostgreSQL):
cursor.execute("INSERT ... RETURNING *")
row = cursor.fetchone()  # Same!

# CHANGE: JSON handling
# Before (SQLite):
json.dumps(raw_response)  # Store as TEXT

# After (PostgreSQL):
raw_response  # JSONB natively supported, no dumps needed

# ADD: user_id parameter to all queries
# Before:
def list_items(category=None, limit=50, offset=0):
    query = "SELECT * FROM items WHERE ..."

# After:
def list_items(user_id: str, category=None, limit=50, offset=0):
    query = "SELECT * FROM items WHERE user_id = %s AND ..."
    params = [user_id]

# CHANGE: FTS search
# Before (SQLite FTS5):
FROM items_fts WHERE items_fts MATCH ?

# After (PostgreSQL tsvector):
FROM analyses WHERE search_vector @@ to_tsquery(?)

# CHANGE: Vector search
# Before (sqlite-vec):
vec_distance(embedding, ?)

# After (pgvector):
embedding <-> ?
```

**Estimate**: ~100-150 lines changed out of ~800 lines (12-18% of file)

---

**3. NEW: `app/middleware/auth.py`**

**Create new file** (~50 lines):
```python
from fastapi import Request, HTTPException
from jose import jwt, JWTError
import requests

COGNITO_JWKS_URL = "https://cognito-idp.{region}.amazonaws.com/{pool}/.well-known/jwks.json"

async def authenticate(request: Request, call_next):
    # Skip health checks
    if request.url.path == "/health":
        return await call_next(request)

    # Extract JWT token
    auth_header = request.headers.get("Authorization")
    if not auth_header:
        raise HTTPException(401, "Missing auth")

    token = auth_header.split(" ")[1]

    # Verify and extract user_id
    claims = jwt.decode(token, ...)
    request.state.user_id = claims["sub"]

    return await call_next(request)
```

---

**4. NEW: `app/config.py`**

**Create new file** (~40 lines):
```python
import boto3
import os

# Load secrets from Parameter Store
def get_secret(name: str) -> str:
    ssm = boto3.client('ssm')
    response = ssm.get_parameter(Name=name, WithDecryption=True)
    return response['Parameter']['Value']

# Set environment variables from secrets
os.environ['ANTHROPIC_API_KEY'] = get_secret('/collections/anthropic-api-key')
os.environ['OPENAI_API_KEY'] = get_secret('/collections/openai-api-key')
os.environ['VOYAGE_API_KEY'] = get_secret('/collections/voyage-api-key')
os.environ['DATABASE_URL'] = get_secret('/collections/database-url')
```

---

#### üì¶ **New Lambda Functions (Extract from existing code)**

**1. Image Processor Lambda** (~80 lines)
- Extract image resizing logic (if you have it) OR add new
- S3 event handler
- Publish EventBridge event

**2. Analyzer Lambda** (~60 lines)
- Reuse `llm.py` (copy/paste!)
- EventBridge handler
- Call `analyze_image()` from existing code

**3. Embedder Lambda** (~50 lines)
- Reuse `embeddings.py` (copy/paste!)
- EventBridge handler
- Call `generate_embedding()` from existing code

---

### Summary: Code Changes by File

| File | Status | Lines Changed | Effort |
|------|--------|---------------|--------|
| `llm.py` | ‚úÖ Copy/paste | 0 | None |
| `embeddings.py` | ‚úÖ Copy/paste | 0 | None |
| `models.py` | ‚úÖ Copy/paste | 0 | None |
| `main.py` | üîß Modify | ~40 / 981 (4%) | Low |
| `database.py` | üîß Modify | ~120 / 800 (15%) | Medium |
| `middleware/auth.py` | ‚ú® New | ~50 | Low |
| `config.py` | ‚ú® New | ~40 | Low |
| `lambdas/image_processor/` | ‚ú® New | ~80 | Low |
| `lambdas/analyzer/` | ‚ú® New (reuses llm.py) | ~60 | Low |
| `lambdas/embedder/` | ‚ú® New (reuses embeddings.py) | ~50 | Low |

**Total new code**: ~280 lines
**Total modified code**: ~160 lines
**Reused unchanged code**: ~1500+ lines

---

### Migration Complexity: **LOW-MEDIUM**

Most of your business logic (LLM analysis, embeddings, search) **works as-is**. The main changes are:
1. Database connection (SQLite ‚Üí PostgreSQL)
2. Authentication (add JWT middleware)
3. File storage (local filesystem ‚Üí S3)
4. Add `user_id` filters to queries

Everything else is **copy/paste**!

---

## Project Structure

```
collections-cloud/
‚îú‚îÄ‚îÄ infrastructure/                  # AWS CDK infrastructure
‚îÇ   ‚îú‚îÄ‚îÄ app.py                      # Main CDK stack definition
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt            # aws-cdk-lib, constructs
‚îÇ   ‚îú‚îÄ‚îÄ cdk.json                    # CDK configuration
‚îÇ   ‚îî‚îÄ‚îÄ README.md                   # Deployment instructions
‚îÇ
‚îú‚îÄ‚îÄ app/                            # API Lambda (FastAPI)
‚îÇ   ‚îú‚îÄ‚îÄ main.py                     # FastAPI app with Mangum handler
‚îÇ   ‚îú‚îÄ‚îÄ database.py                 # PostgreSQL connection + RLS
‚îÇ   ‚îú‚îÄ‚îÄ llm.py                      # AI analysis (existing code)
‚îÇ   ‚îú‚îÄ‚îÄ embeddings.py               # Vector generation (existing code)
‚îÇ   ‚îú‚îÄ‚îÄ config.py                   # NEW: Secrets/config management
‚îÇ   ‚îú‚îÄ‚îÄ middleware/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ auth.py                 # NEW: Cognito JWT validation
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile                  # Container for Lambda
‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt
‚îÇ
‚îú‚îÄ‚îÄ lambdas/
‚îÇ   ‚îú‚îÄ‚îÄ image_processor/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ handler.py              # S3 event handler
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt        # Pillow, boto3
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ analyzer/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ handler.py              # EventBridge handler
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile              # Shares modules from app/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ embedder/
‚îÇ       ‚îú‚îÄ‚îÄ handler.py              # EventBridge handler
‚îÇ       ‚îî‚îÄ‚îÄ requirements.txt        # voyageai, boto3, psycopg2
‚îÇ
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ deploy.sh                   # CDK deployment script
‚îÇ   ‚îú‚îÄ‚îÄ populate_secrets.sh         # Populate Parameter Store
‚îÇ   ‚îú‚îÄ‚îÄ migrate_sqlite_to_postgres.py
‚îÇ   ‚îî‚îÄ‚îÄ migrate_images_to_s3.py
‚îÇ
‚îú‚îÄ‚îÄ data/                           # Local development only
‚îÇ   ‚îú‚îÄ‚îÄ collections.db              # Will be migrated to RDS
‚îÇ   ‚îú‚îÄ‚îÄ collections_golden.db       # Will be migrated to RDS
‚îÇ   ‚îî‚îÄ‚îÄ images/                     # Will be migrated to S3
‚îÇ
‚îî‚îÄ‚îÄ AWS_MIGRATION_PLAN.md           # This document
```

---

## CDK Infrastructure Code

### Main Stack (infrastructure/app.py)

Key components that CDK will create:

1. **RDS PostgreSQL** - db.t4g.micro (public) with automatic backups, SSL required
2. **S3 Bucket** - Images with EventBridge notifications enabled
3. **Cognito User Pool** - User authentication
4. **API Gateway** (HTTP API) - Routes to API Lambda
5. **4 Lambda Functions** - API, Image Processor, Analyzer, Embedder
6. **EventBridge Rules** - Workflow automation
7. **IAM Roles** - Least-privilege permissions
8. **Parameter Store Parameters** - Secrets storage (populated manually)
9. **CloudWatch Log Groups** - Centralized logging
10. **Security Groups** - RDS accessible only from Lambda + your IP

---

## Authentication & Multi-Tenancy Implementation

### Simplified Approach (No RLS, No Users Table)

**Middleware extracts user_id from JWT:**

```python
# app/middleware/auth.py
from fastapi import Request, HTTPException
from jose import jwt, JWTError
import requests

COGNITO_REGION = "us-east-1"
COGNITO_USER_POOL_ID = "us-east-1_XXXXXXXXX"
COGNITO_JWKS_URL = f"https://cognito-idp.{COGNITO_REGION}.amazonaws.com/{COGNITO_USER_POOL_ID}/.well-known/jwks.json"

@app.middleware("http")
async def authenticate(request: Request, call_next):
    # Skip auth for health checks
    if request.url.path in ["/health", "/docs"]:
        return await call_next(request)

    # Extract token
    auth_header = request.headers.get("Authorization")
    if not auth_header or not auth_header.startswith("Bearer "):
        raise HTTPException(status_code=401, detail="Missing authorization")

    token = auth_header.split(" ")[1]

    # Verify JWT and extract user_id
    try:
        # Download JWKS (cache this in production)
        jwks = requests.get(COGNITO_JWKS_URL).json()

        # Decode and verify token
        claims = jwt.decode(
            token,
            jwks,
            algorithms=["RS256"],
            audience=COGNITO_CLIENT_ID
        )

        # Store user_id in request state
        request.state.user_id = claims["sub"]  # Cognito user UUID

    except JWTError:
        raise HTTPException(status_code=401, detail="Invalid token")

    return await call_next(request)
```

**Add user_id to all database queries:**

```python
# app/main.py

@app.get("/items")
async def get_items(request: Request, db: Database = Depends(get_db)):
    user_id = request.state.user_id

    items = await db.fetch_all(
        "SELECT * FROM items WHERE user_id = :user_id ORDER BY created_at DESC",
        {"user_id": user_id}
    )
    return items

@app.post("/items/{item_id}/analyze")
async def analyze_item(item_id: str, request: Request, db: Database = Depends(get_db)):
    user_id = request.state.user_id

    # Verify item belongs to user
    item = await db.fetch_one(
        "SELECT * FROM items WHERE id = :item_id AND user_id = :user_id",
        {"item_id": item_id, "user_id": user_id}
    )
    if not item:
        raise HTTPException(status_code=404, detail="Item not found")

    # Proceed with analysis...
```

**Vector search with user isolation:**

```python
@app.post("/search")
async def search(query: str, request: Request, db: Database = Depends(get_db)):
    user_id = request.state.user_id

    # Generate query embedding
    query_embedding = await generate_embedding(query)

    # Search only user's embeddings
    results = await db.fetch_all("""
        SELECT e.*, i.filename, a.summary
        FROM embeddings e
        JOIN items i ON e.item_id = i.id
        JOIN analyses a ON e.analysis_id = a.id
        WHERE e.user_id = :user_id
        ORDER BY e.embedding <-> :query_vector
        LIMIT 10
    """, {"user_id": user_id, "query_vector": query_embedding})

    return results
```

**Logging with user context:**

```python
import logging

logger = logging.getLogger(__name__)

@app.post("/items")
async def create_item(request: Request, file: UploadFile, db: Database = Depends(get_db)):
    user_id = request.state.user_id

    # Log with user context
    logger.info(
        "Creating item",
        extra={"user_id": user_id, "filename": file.filename}
    )

    # Create item...
    await db.execute(
        "INSERT INTO items (id, user_id, filename, ...) VALUES (:id, :user_id, ...)",
        {"id": item_id, "user_id": user_id, ...}
    )
```

---

## Migration Phases

### Phase 1: Infrastructure Setup & Validation

**Goal**: Deploy AWS infrastructure with CDK and validate each component

Tasks:
1. Install CDK and bootstrap AWS account
2. Create CDK stack definition (infrastructure/app.py)
   - Public RDS PostgreSQL with security groups
   - S3 bucket with EventBridge notifications
   - Cognito User Pool
   - API Gateway + Lambda functions
3. Deploy infrastructure: `cdk deploy`
4. **Test each component** (see testing checklist below)
5. Populate Parameter Store with API keys
6. Enable pgvector extension on RDS
7. Run schema creation scripts (simplified, no RLS)

**Deliverable**: Fully deployed and validated AWS infrastructure

---

### Infrastructure Testing Checklist

Run these tests after `cdk deploy` to validate infrastructure before migrating data:

#### 1. RDS PostgreSQL Connection Test
```bash
# From your local machine (add your IP to security group first)
psql -h <rds-endpoint>.rds.amazonaws.com -U postgres -d collections

# Test basic operations
CREATE TABLE test_table (id TEXT PRIMARY KEY, data TEXT);
INSERT INTO test_table VALUES ('test1', 'hello');
SELECT * FROM test_table;
DROP TABLE test_table;

# Test pgvector extension
CREATE EXTENSION IF NOT EXISTS vector;
CREATE TABLE test_embeddings (id TEXT, vec vector(3));
INSERT INTO test_embeddings VALUES ('1', '[1,2,3]');
SELECT * FROM test_embeddings ORDER BY vec <-> '[1,2,4]' LIMIT 1;
DROP TABLE test_embeddings;

# Verify SSL connection
\conninfo  # Should show SSL connection
```

**Expected result**: Successful connection, table operations work, pgvector works

---

#### 2. Parameter Store Test
```bash
# Populate test secret
aws ssm put-parameter \
  --name /collections/test-secret \
  --value "test-value" \
  --type SecureString

# Read it back
aws ssm get-parameter \
  --name /collections/test-secret \
  --with-decryption \
  --query 'Parameter.Value' \
  --output text

# Delete test parameter
aws ssm delete-parameter --name /collections/test-secret
```

**Expected result**: Can store and retrieve encrypted parameters

---

#### 3. Cognito User Pool Test
```bash
# Create test user
aws cognito-idp sign-up \
  --client-id <app-client-id> \
  --username testuser@example.com \
  --password TempPassword123! \
  --user-attributes Name=email,Value=testuser@example.com

# Confirm user (admin)
aws cognito-idp admin-confirm-sign-up \
  --user-pool-id <user-pool-id> \
  --username testuser@example.com

# Get JWT token
aws cognito-idp initiate-auth \
  --auth-flow USER_PASSWORD_AUTH \
  --client-id <app-client-id> \
  --auth-parameters USERNAME=testuser@example.com,PASSWORD=TempPassword123!

# Decode JWT and verify 'sub' claim
echo "<id-token>" | cut -d'.' -f2 | base64 -d | jq .sub
```

**Expected result**: User created, JWT token obtained, `sub` claim present

---

#### 4. S3 Bucket Test
```bash
# Upload test file
echo "test content" > test.txt
aws s3 cp test.txt s3://<bucket-name>/test-user-id/images/test.txt

# Download it back
aws s3 cp s3://<bucket-name>/test-user-id/images/test.txt downloaded.txt
cat downloaded.txt

# Check EventBridge notifications are enabled
aws s3api get-bucket-notification-configuration --bucket <bucket-name>

# Clean up
aws s3 rm s3://<bucket-name>/test-user-id/images/test.txt
rm test.txt downloaded.txt
```

**Expected result**: Upload/download works, EventBridge config present

---

#### 5. Lambda Function Test (Hello World)

**Deploy minimal test Lambda first:**

```python
# lambdas/test/handler.py
import json

def handler(event, context):
    return {
        "statusCode": 200,
        "body": json.dumps({"message": "Hello from Lambda!"})
    }
```

```bash
# Invoke Lambda directly (via AWS CLI)
aws lambda invoke \
  --function-name collections-test-lambda \
  --payload '{}' \
  response.json

cat response.json
```

**Expected result**: Lambda returns 200 with test message

---

#### 6. Lambda + RDS Connection Test

**Test Lambda can connect to RDS:**

```python
# lambdas/test_db/handler.py
import psycopg2
import os

def handler(event, context):
    try:
        conn = psycopg2.connect(
            host=os.environ['DB_HOST'],
            database=os.environ['DB_NAME'],
            user=os.environ['DB_USER'],
            password=os.environ['DB_PASSWORD']
        )
        cursor = conn.cursor()
        cursor.execute("SELECT version();")
        version = cursor.fetchone()
        conn.close()

        return {
            "statusCode": 200,
            "body": f"Connected! PostgreSQL version: {version[0]}"
        }
    except Exception as e:
        return {
            "statusCode": 500,
            "body": f"Error: {str(e)}"
        }
```

**Expected result**: Lambda successfully connects to RDS

---

#### 7. Lambda + Parameter Store Test

**Test Lambda can read secrets:**

```python
# lambdas/test_secrets/handler.py
import boto3
import json

def handler(event, context):
    ssm = boto3.client('ssm')

    try:
        response = ssm.get_parameter(
            Name='/collections/test-secret',
            WithDecryption=True
        )

        return {
            "statusCode": 200,
            "body": json.dumps({"secret": response['Parameter']['Value']})
        }
    except Exception as e:
        return {
            "statusCode": 500,
            "body": json.dumps({"error": str(e)})
        }
```

**Expected result**: Lambda can read encrypted parameters

---

#### 8. API Gateway + Lambda Test

```bash
# Test API Gateway endpoint
curl -X GET <api-gateway-url>/health

# Should return Lambda response
```

**Expected result**: API Gateway routes to Lambda correctly

---

#### 9. EventBridge + Lambda Test

**Test S3 ‚Üí EventBridge ‚Üí Lambda workflow:**

```python
# lambdas/test_event/handler.py
import json

def handler(event, context):
    print(f"Received event: {json.dumps(event)}")
    return {"statusCode": 200, "body": "Event received"}
```

```bash
# Upload file to S3 (should trigger EventBridge)
aws s3 cp test.txt s3://<bucket-name>/test/test.txt

# Check CloudWatch Logs for Lambda execution
aws logs tail /aws/lambda/collections-image-processor --follow
```

**Expected result**: S3 upload triggers Lambda via EventBridge

---

#### 10. End-to-End Integration Test

**Final validation before data migration:**

```bash
# 1. Get Cognito JWT token (from test #3)
TOKEN="<jwt-token>"

# 2. Call API with authentication
curl -X GET <api-gateway-url>/health \
  -H "Authorization: Bearer $TOKEN"

# 3. Test database query via API
curl -X GET <api-gateway-url>/items \
  -H "Authorization: Bearer $TOKEN"

# Expected: Empty array [] (no data yet, but connection works)
```

**Expected result**: Authenticated API call returns valid response

---

### Testing Script

Create `scripts/test_infrastructure.sh`:

```bash
#!/bin/bash
set -e

echo "üîç Testing AWS Infrastructure..."

# Load environment variables from CDK outputs
export REGION="us-east-1"
export DB_ENDPOINT=$(aws cloudformation describe-stacks \
  --stack-name CollectionsStack \
  --query 'Stacks[0].Outputs[?OutputKey==`RDSEndpoint`].OutputValue' \
  --output text)

export API_URL=$(aws cloudformation describe-stacks \
  --stack-name CollectionsStack \
  --query 'Stacks[0].Outputs[?OutputKey==`ApiUrl`].OutputValue' \
  --output text)

# Run tests
echo "‚úÖ 1. Testing RDS connection..."
psql -h $DB_ENDPOINT -U postgres -d collections -c "SELECT 1;"

echo "‚úÖ 2. Testing Parameter Store..."
aws ssm put-parameter --name /collections/test --value "test" --type String --overwrite
aws ssm get-parameter --name /collections/test

echo "‚úÖ 3. Testing Lambda invoke..."
aws lambda invoke --function-name collections-api-lambda --payload '{}' /tmp/response.json
cat /tmp/response.json

echo "‚úÖ 4. Testing API Gateway..."
curl -f $API_URL/health

echo ""
echo "üéâ All infrastructure tests passed!"
```

**Run**: `chmod +x scripts/test_infrastructure.sh && ./scripts/test_infrastructure.sh`

---

### Phase 2: Database Migration

**Goal**: Migrate SQLite to PostgreSQL with simplified schema

Tasks:
1. Export SQLite data to CSV/JSON
2. Create test user in Cognito (use their `sub` as user_id for existing data)
3. Transform schema:
   - Add `user_id` columns (TEXT, no foreign key)
   - Change `raw_response` and `embedding_source` from TEXT ‚Üí JSONB
   - Add indexes on `user_id` columns
4. Import data into RDS PostgreSQL
5. Test queries with manual `WHERE user_id = ?` filters
6. Test FTS search (PostgreSQL tsvector)
7. Test vector search (pgvector)

**Deliverable**: PostgreSQL with migrated data, no RLS complexity

---

### Phase 3: Code Adaptation

**Goal**: Adapt existing code for Lambda and multi-tenancy

Tasks:
1. Create `config.py` for secrets management (Parameter Store)
2. Update `database.py` for PostgreSQL connections
3. Add `middleware/auth.py` for Cognito JWT validation
   - Extract `sub` from JWT ‚Üí store in `request.state.user_id`
4. Update ALL endpoints to add `WHERE user_id = ?` filters
   - GET /items ‚Üí filter by user_id
   - POST /items ‚Üí insert with user_id
   - Search endpoints ‚Üí filter by user_id
5. Add Mangum adapter to FastAPI (ASGI ‚Üí Lambda)
6. Create Dockerfile for API Lambda
7. Update search functions (FTS5 ‚Üí PostgreSQL tsvector)
8. Update vector search (sqlite-vec ‚Üí pgvector with user filtering)

**Deliverable**: Lambda-ready FastAPI application with manual multi-tenancy

---

### Phase 4: Lambda Functions (Week 2-3)

**Goal**: Create event-driven Lambda functions

Tasks:
1. Create Image Processor Lambda (resize, thumbnail)
2. Create Analyzer Lambda (vision AI analysis)
3. Create Embedder Lambda (vector generation)
4. Set up EventBridge event schemas
5. Test S3 ‚Üí Image Processor trigger
6. Test EventBridge ‚Üí Analyzer trigger
7. Test EventBridge ‚Üí Embedder trigger
8. Test manual Lambda invocations from API

**Deliverable**: 4 working Lambda functions with event-driven workflow

---

### Phase 5: Storage Migration (Week 3)

**Goal**: Migrate images from local filesystem to S3

Tasks:
1. Write migration script (local ‚Üí S3)
2. Organize S3 structure: `{user_id}/images/{uuid}.jpg`
3. Upload existing images with proper metadata
4. Update database `file_path` columns
5. Generate pre-signed URLs for image access
6. Update frontend to use S3 URLs
7. Delete local image files

**Deliverable**: All images in S3 with proper isolation

---

### Phase 6: Authentication (Week 3-4)

**Goal**: Implement user authentication and signup

Tasks:
1. Create Cognito User Pool (done via CDK)
2. Build signup/login UI (or use Cognito Hosted UI)
3. Integrate Cognito SDK in frontend
4. Implement JWT storage (localStorage)
5. Add Authorization header to all API requests
6. Test signup flow
7. Test login flow
8. Test multi-user data isolation

**Deliverable**: Working user authentication

---

### Phase 7: Testing & Optimization (Week 4)

**Goal**: Optimize performance and test thoroughly

Tasks:
1. Load test API endpoints
2. Optimize Lambda cold starts (container layers, SnapStart)
3. Add connection pooling for RDS
4. Implement caching (ElastiCache or in-memory)
5. Test concurrent users
6. Monitor costs and optimize
7. Set up CloudWatch alarms
8. Test golden dataset functionality

**Deliverable**: Production-ready application

---

### Phase 8: Deployment & Cutover (Week 5)

**Goal**: Launch to users

Tasks:
1. Create production environment (separate CDK stack)
2. Deploy frontend to S3 + CloudFront (optional)
3. Configure custom domain (Route 53)
4. Set up SSL certificate (ACM)
5. Final data migration
6. User onboarding documentation
7. Monitor for issues
8. Decommission local environment

**Deliverable**: Live production system

---

## Cost Optimization Strategies

### Use Free Tiers
- ‚úÖ Lambda: 1M requests/month free
- ‚úÖ Cognito: 50K MAU free
- ‚úÖ Parameter Store: Unlimited standard parameters
- ‚úÖ CloudWatch: 5GB logs free
- ‚úÖ S3: 5GB free (first 12 months)

### Right-Size Resources
- ‚úÖ RDS: Start with db.t4g.micro ($15/month), public access
- ‚úÖ Lambda: 1024-2048MB memory (pay per use), no VPC (faster cold starts)
- ‚úÖ DynamoDB: On-demand billing ($1-5/month for conversation checkpoints)
- ‚úÖ EventBridge: Cron rules for cleanup ($0.10/month)
- ‚úÖ No VPC/NAT Gateway: Saves $32/month
- ‚úÖ No RDS Proxy: Saves $11/month (DynamoDB handles connections natively)

### Security Without VPC Costs
- ‚úÖ Security Groups: Whitelist only Lambda IPs + your dev IP
- ‚úÖ SSL/TLS: Force encrypted connections to RDS
- ‚úÖ IAM: Least-privilege roles for Lambda functions
- ‚úÖ Secrets: Encrypted parameters in Parameter Store

### Monitoring
- Set up billing alarms ($50/month threshold for updated architecture)
- Enable Cost Explorer
- Review monthly costs and optimize
- Monitor DynamoDB usage (should stay < $5/month with proper TTL)
- Track Lambda invocation counts (chat endpoints may increase usage)

---

## Best Practices Validation

This section validates that the migration plan follows best practices across all relevant domains.

### Python Best Practices ‚úÖ

- [x] **Type Hints**: All functions use proper type annotations (Pydantic models throughout)
- [x] **Async/Await**: I/O operations use async patterns where beneficial
- [x] **Context Managers**: Database connections and resources use context managers
- [x] **Environment Variables**: Configuration via env vars (12-factor app)
- [x] **Logging**: Structured logging with appropriate levels
- [x] **Error Handling**: Specific exceptions, proper error propagation
- [x] **Virtual Environments**: Dependency isolation (requirements.txt)
- [x] **Code Organization**: Clear module structure, separation of concerns

**Examples in Codebase:**
- `database.py`: Context managers for DB connections
- `models.py`: Comprehensive Pydantic models with validation
- `llm.py`: Async operations for API calls
- `config/*.py`: Configuration modules for different environments

### LangChain Best Practices ‚úÖ

- [x] **BaseRetriever Pattern**: All retrievers extend `BaseRetriever`
- [x] **Document Format**: Standardized `Document` objects with metadata
- [x] **Embedding Abstraction**: Model-agnostic embedding interface
- [x] **Chain Composition**: Modular, reusable components
- [x] **Metadata Filtering**: Category and user-based filtering support
- [x] **Batch Operations**: Efficient batch embedding generation

**Examples in Codebase:**
- `retrieval/langchain_retrievers.py`: Three retrievers implementing `BaseRetriever`
- `retrieval/chroma_manager.py` ‚Üí `pgvector_manager.py`: Embedding abstraction
- `HybridLangChainRetriever`: Sophisticated RRF composition

### LangGraph Best Practices ‚úÖ

- [x] **Checkpointer Abstraction**: Clean separation between state and persistence
- [x] **Thread-Based Tracking**: Conversation continuity via thread IDs
- [x] **Streaming Support**: Real-time response streaming
- [x] **Tool-Based Architecture**: ReAct pattern with explicit tools
- [x] **Configurable Limits**: Recursion limits prevent runaway costs
- [x] **State Serialization**: Proper checkpoint save/load cycle

**Examples in Codebase:**
- `chat/agentic_chat.py`: `create_react_agent` with tools
- `chat/conversation_manager.py`: Checkpointer interface (SqliteSaver ‚Üí DynamoDBCheckpointer)
- `config/chat_config.py`: `CHAT_MAX_ITERATIONS = 3` (cost control)

**AWS Migration:**
```python
# DynamoDB checkpointer maintains LangGraph interface
class DynamoDBCheckpointer(BaseCheckpointSaver):
    def put(self, config, checkpoint, metadata):
        # Save to DynamoDB with TTL
        ...

    def get(self, config):
        # Load from DynamoDB
        ...
```

### LangSmith Best Practices ‚úÖ

- [x] **Tracing Decorator**: `@traceable` on all key operations
- [x] **Trace ID Persistence**: Stored with analysis results for correlation
- [x] **Prompt Management**: Prompts fetched from LangSmith Hub with fallback
- [x] **Evaluation Datasets**: Curated datasets for quality tracking
- [x] **Custom Evaluators**: Domain-specific evaluation logic
- [x] **Token Tracking**: Automatic via LangChain Chat models

**Examples in Codebase:**
- `llm.py`: All analysis functions use `@traceable`
- Database stores `trace_id` column
- `evaluation/langsmith_evaluators.py`: Custom evaluators
- `evaluation/langsmith_dataset.py`: Dataset management

**AWS Migration:**
- No changes needed - LangSmith is cloud-based
- Ensure `LANGSMITH_API_KEY` in Parameter Store
- Environment variables remain the same

### AWS Serverless Best Practices ‚úÖ

- [x] **Stateless Functions**: Lambda functions are stateless (state in DynamoDB/RDS)
- [x] **Cold Start Optimization**: No VPC (faster cold starts), minimal dependencies
- [x] **Pay-Per-Use Pricing**: Lambda, DynamoDB, API Gateway all on-demand
- [x] **Managed Services**: Use RDS, DynamoDB, S3 instead of self-managed
- [x] **Auto-Scaling**: All services scale automatically
- [x] **Security**: IAM roles, encryption at rest, SSL in transit
- [x] **Monitoring**: CloudWatch Logs, metrics, alarms

**Architecture Decisions:**
- ‚úÖ No VPC: Saves $32/month, faster cold starts
- ‚úÖ DynamoDB for state: Serverless, auto-scaling
- ‚úÖ Public RDS: Simpler than VPC + RDS Proxy
- ‚úÖ S3 for storage: Serverless, durable
- ‚úÖ EventBridge for events: Serverless orchestration

### AWS Cost Optimization Best Practices ‚úÖ

- [x] **Right-Sizing**: db.t4g.micro (smallest suitable instance)
- [x] **On-Demand Pricing**: No upfront commitment (DynamoDB, Lambda)
- [x] **Free Tier Usage**: Parameter Store, Cognito, Lambda (1M requests)
- [x] **TTL-Based Cleanup**: No manual cleanup Lambda cost (DynamoDB TTL is free)
- [x] **Billing Alarms**: Prevent cost surprises ($50/month threshold)
- [x] **Cost Tagging**: Tag all resources for cost allocation

**Cost Breakdown (Updated):**
| Service | Original Plan | Updated Plan | Delta |
|---------|--------------|--------------|-------|
| RDS PostgreSQL | $15-20 | $15-20 | $0 |
| Lambda | $5-15 | $8-20 | +$3-5 |
| API Gateway | $1-2 | $2-3 | +$1 |
| DynamoDB | N/A | $1-5 | +$1-5 |
| EventBridge | $0 | $0.10 | +$0.10 |
| S3 | $0.50 | $0.50 | $0 |
| CloudWatch | $1-2 | $2-3 | +$1 |
| **TOTAL** | **$22-40** | **$29-52** | **+$7-12** |

**Justification**: +$7-12/month for multi-turn chat is acceptable for the added value.

### Infrastructure as Code Best Practices ‚úÖ

- [x] **AWS CDK**: Python-based, type-safe IaC
- [x] **Version Control**: Infrastructure code in Git
- [x] **Reproducible**: Single command deployment (`cdk deploy`)
- [x] **Environment Separation**: Dev/staging/prod via CDK contexts
- [x] **Outputs**: CDK outputs for endpoint URLs, ARNs
- [x] **Destroy Support**: `cdk destroy` for clean teardown

**CDK Structure:**
```
infrastructure/
‚îú‚îÄ‚îÄ app.py                 # Main stack definition
‚îú‚îÄ‚îÄ requirements.txt       # CDK dependencies
‚îú‚îÄ‚îÄ cdk.json              # CDK configuration
‚îî‚îÄ‚îÄ README.md             # Deployment instructions
```

### Security Best Practices ‚úÖ

- [x] **Least Privilege IAM**: Each Lambda has minimal required permissions
- [x] **Secrets Encryption**: Parameter Store SecureString (KMS)
- [x] **SSL/TLS**: Enforced for RDS connections
- [x] **JWT Validation**: Cognito tokens verified with JWKS
- [x] **User Isolation**: Manual `WHERE user_id = ?` filters
- [x] **Security Groups**: Whitelist only required sources
- [x] **No Hardcoded Secrets**: All secrets from Parameter Store

**Multi-Tenancy Security:**
```python
# JWT middleware extracts user_id
user_id = jwt.decode(token)['sub']

# All queries filtered by user_id
SELECT * FROM items WHERE user_id = :user_id

# DynamoDB thread_id includes user_id
thread_id = f"{user_id}#{session_id}"
```

### Data Management Best Practices ‚úÖ

- [x] **Database Normalization**: Proper foreign keys, CASCADE deletes
- [x] **JSONB for Flexibility**: Semi-structured data in JSONB columns
- [x] **Indexes**: Strategic indexes on filter columns (user_id, category)
- [x] **Vector Indexes**: IVFFlat for pgvector performance
- [x] **Full-Text Search**: GIN index for tsvector
- [x] **Batch Operations**: Efficient bulk inserts/updates

**Schema Design:**
- ‚úÖ Foreign keys with CASCADE DELETE
- ‚úÖ User isolation via user_id column (indexed)
- ‚úÖ JSONB for raw_response (flexible schema)
- ‚úÖ tsvector for search (auto-updated via trigger)
- ‚úÖ pgvector with cosine similarity

### Migration Best Practices ‚úÖ

- [x] **Backward Compatibility**: Keep SQLite/ChromaDB as backup
- [x] **Validation Scripts**: Automated migration verification
- [x] **Rollback Plan**: Clear rollback procedures documented
- [x] **Incremental Migration**: Phase-based approach (7-day plan)
- [x] **Testing at Each Phase**: Validate before proceeding
- [x] **Data Integrity Checks**: Count comparisons, sample queries

**Validation Example:**
```python
# Compare counts
assert sqlite_count == postgres_count
assert chroma_count == pgvector_count

# Compare sample results
overlap = len(sqlite_results & postgres_results)
assert overlap >= 0.8 * len(sqlite_results)
```

### Summary

**Overall Assessment**: ‚úÖ **EXCELLENT**

The migration plan follows best practices across all domains:
- ‚úÖ Python ecosystem (type hints, async, modern patterns)
- ‚úÖ LangChain/LangGraph (official patterns, proper abstractions)
- ‚úÖ LangSmith (tracing, evaluation, prompt management)
- ‚úÖ AWS (serverless-first, cost-optimized, secure)
- ‚úÖ Infrastructure as Code (CDK, reproducible, version-controlled)

**Key Strengths**:
1. Pragmatic cost optimization ($29-52/month vs typical $100-200)
2. Explicit multi-tenancy (debuggable, auditable)
3. Serverless architecture (scales automatically)
4. DynamoDB for checkpoints (optimal for Lambda)
5. pgvector for embeddings (2.4x faster than ChromaDB, free)

**Risk Mitigation**:
- Clear rollback procedures
- Validation at each phase
- Backup retention
- Incremental migration

---

## Rollback Plan

If migration fails or issues arise:

1. **Database**: Keep SQLite backups, can restore locally
2. **Images**: Keep local copies until S3 is verified
3. **Code**: Use Git branches (main = local, cloud = AWS)
4. **Infrastructure**: `cdk destroy` removes all AWS resources
5. **Cost**: Delete RDS, Lambdas to stop charges immediately

---

## Golden Dataset Strategy

**Recommended approach**: Test user account

1. Create `test@collections.local` user in Cognito
2. Upload curated golden dataset images to that user's account
3. Run evaluations filtered by `user_id = test-user-uuid`
4. Keeps everything in single database (no dual-database complexity)
5. FREE (no extra infrastructure)

**Alternative**: Separate RDS instance for testing (~$15/month)

---

## Key Decisions Made

| Decision | Choice | Rationale |
|----------|--------|-----------|
| Cloud Provider | AWS | User has AWS experience, provider-native, mature serverless |
| Compute | Lambda | Cost-effective for bursty usage, auto-scales, pay-per-use |
| Database | RDS PostgreSQL (public) | JSONB for documents, pgvector for vectors, no VPC costs |
| Multi-tenancy | Manual WHERE filters | Simpler than RLS, easier to debug, no session variables |
| Users Table | None | Use Cognito `sub` directly, no sync complexity |
| Auth | Cognito | Provider-native, free tier, JWT tokens, MFA support |
| Secrets | Parameter Store | FREE vs $1.60/month for Secrets Manager |
| IaC | AWS CDK (Python) | Python-based, type-safe, AWS-official, higher-level than CloudFormation |
| API Gateway | HTTP API | Cheaper than REST API ($1/M vs $3.50/M requests) |
| Vector Search | pgvector | Keep search in-database, simpler than separate service |
| BM25 Search | PostgreSQL FTS | Good enough, avoids Elasticsearch costs |
| Network | Public RDS | Saves $32/month (NAT Gateway), simpler architecture |

---

## Development Tooling & Automation

### Overview

Comprehensive development scripts and Makefile to streamline AWS infrastructure management, deployment, testing, and data seeding.

### Tooling Philosophy

**Design Principles**:
- ‚úÖ Single entry point (Makefile) for all operations
- ‚úÖ Focus on dev environment initially (can expand to staging/prod later)
- ‚úÖ Leverage existing automation (evaluate_retrieval.py, export_db.py)
- ‚úÖ Fast iteration (quick Lambda updates without full CDK deploy)
- ‚úÖ Safety first (confirmations, dry-run modes, rollback support)

**User Decisions**:
- Dev environment only initially
- Local scripts only (no CI/CD)
- Scripted secrets population (from .env)
- Extend existing `evaluate_retrieval.py` for AWS

### Makefile - Primary Developer Interface

**Key Make Targets**:

```makefile
# Infrastructure Management
make infra-bootstrap       # CDK bootstrap AWS account
make infra-deploy          # Deploy CDK stack
make infra-diff            # Show infrastructure changes before deploy
make infra-destroy         # Destroy stack (with safety confirmation)

# Secrets Management
make secrets-populate      # Push secrets from .env to Parameter Store
make secrets-export        # Pull secrets from Parameter Store to local .env

# Database Operations
make db-connect            # Open psql connection to RDS
make db-migrate            # Run schema migrations
make db-seed-golden        # Seed golden dataset (55 items from existing golden DB)
make db-seed-full          # Seed full production data
make db-reset              # Drop and recreate schema (with confirmation)

# Lambda Deployment
make lambda-deploy-api     # Deploy API Lambda only (fast, 2-minute update)
make lambda-deploy-all     # Deploy all Lambda functions
make lambda-logs FUNC=api  # Tail CloudWatch logs for specific function

# Cognito User Management
make cognito-create-user   # Create single test user, save token
make cognito-get-token     # Get fresh JWT token from existing user

# S3 Image Management
make s3-upload-images      # Upload local images to S3 with user_id prefix
make s3-download-images    # Download S3 images locally

# Testing & Validation
make test-infra            # Run 10-step infrastructure validation
make test-api              # Test all API endpoints with auth
make test-e2e              # End-to-end workflow test
make test-all              # Run all tests and generate report

# Evaluation & Comparison
make eval-aws              # Run retrieval evaluation against AWS API
make eval-compare          # Compare local vs AWS evaluation metrics

# Data Migration
make migrate-db            # Migrate SQLite ‚Üí PostgreSQL
make migrate-images        # Migrate local images ‚Üí S3
make migrate-validate      # Verify migration data integrity

# Utilities
make status                # Show current AWS resources and their status
make clean                 # Clean local build artifacts
make help                  # Display all available commands
```

**Features**:
- Color-coded output (green=success, red=error, yellow=warning)
- Prerequisite checking (AWS CLI, CDK, psql, jq, python)
- Safety confirmations for destructive operations
- Auto-loads environment from CDK outputs

### Scripts Organization

```
scripts/
‚îú‚îÄ‚îÄ aws/                              # AWS-specific automation
‚îÇ   ‚îú‚îÄ‚îÄ infra/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ deploy.sh                 # CDK deploy wrapper with safety checks
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ destroy.sh                # CDK destroy with confirmation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ diff.sh                   # Show infrastructure changes
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ bootstrap.sh              # CDK bootstrap
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ lambda/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ deploy-api.sh             # Deploy API Lambda only (fast)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ deploy-all.sh             # Deploy all Lambda functions
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ logs.sh                   # Tail CloudWatch logs
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ db/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ connect.sh                # psql connection helper
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ seed.sh                   # Seed test data (golden/full/schema)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ reset.sh                  # Drop + recreate schema
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ cognito/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ create-user.sh            # Create test user, get JWT token
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ get-token.sh              # Get fresh JWT token
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ s3/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ upload-images.sh          # Upload local images to S3
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ download-images.sh        # Download S3 images locally
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ secrets/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ populate.sh               # Push secrets to Parameter Store
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ export.sh                 # Pull secrets to .env
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ test/
‚îÇ       ‚îú‚îÄ‚îÄ test-infrastructure.sh    # 10-step infrastructure validation
‚îÇ       ‚îú‚îÄ‚îÄ test-api.sh               # API endpoint tests with auth
‚îÇ       ‚îî‚îÄ‚îÄ test-e2e.sh               # End-to-end workflow test
‚îÇ
‚îú‚îÄ‚îÄ migrate/                          # Migration utilities
‚îÇ   ‚îú‚îÄ‚îÄ sqlite-to-postgres.py         # SQLite ‚Üí PostgreSQL with transformations
‚îÇ   ‚îú‚îÄ‚îÄ images-to-s3.py               # Local filesystem ‚Üí S3
‚îÇ   ‚îî‚îÄ‚îÄ validate-migration.py         # Verify data integrity post-migration
‚îÇ
‚îî‚îÄ‚îÄ [existing scripts remain]
    ‚îú‚îÄ‚îÄ export_db.py                  # Keep existing
    ‚îú‚îÄ‚îÄ import_db.py                  # Keep existing
    ‚îú‚îÄ‚îÄ evaluate_retrieval.py         # Extend for AWS (see below)
    ‚îî‚îÄ‚îÄ setup_golden_db.py            # Keep existing
```

### Key Scripts Detailed

#### 1. scripts/aws/infra/deploy.sh

**Purpose**: Safe CDK deployment with validation

```bash
#!/bin/bash
# Features:
# - Check AWS credentials
# - Show stack diff before deploy
# - Confirm deployment
# - Capture CDK outputs (RDS endpoint, API URL, bucket name)
# - Save outputs to .aws-outputs.json for other scripts

# Usage:
./scripts/aws/infra/deploy.sh

# Output example:
# ‚úÖ AWS credentials valid
# üìã Stack changes:
# [CDK diff output]
#
# Deploy these changes? [y/N]: y
# üöÄ Deploying stack...
# ‚úÖ Stack deployed successfully
# üìù Saved outputs to .aws-outputs.json
```

#### 2. scripts/aws/secrets/populate.sh

**Purpose**: Populate Parameter Store from .env

```bash
#!/bin/bash
# Features:
# - Read secrets from .env file
# - Create/update Parameter Store parameters
# - Use SecureString encryption
# - Safety: Check if parameters exist, confirm overwrite
# - Support for dry-run mode

# Secrets to populate:
# - /collections/anthropic-api-key
# - /collections/openai-api-key
# - /collections/voyage-api-key
# - /collections/langsmith-api-key
# - /collections/database-url (auto-generated by RDS)

# Usage:
./scripts/aws/secrets/populate.sh            # Interactive mode
./scripts/aws/secrets/populate.sh --dry-run  # Preview only
./scripts/aws/secrets/populate.sh --force    # Overwrite existing
```

#### 3. scripts/aws/db/seed.sh

**Purpose**: Seed RDS database with test data

```bash
#!/bin/bash
# Options:
# --schema-only   : Just create tables (empty database)
# --golden        : Seed with 55-item golden dataset
# --full          : Seed with all production data

# Workflow:
# 1. Connect to RDS (credentials from Parameter Store)
# 2. Create test user in Cognito (if needed), get 'sub'
# 3. Import data with user_id from Cognito sub
# 4. Verify counts match source

# Usage:
./scripts/aws/db/seed.sh --golden

# Output:
# üîë Using Cognito user: testuser@example.com (sub: 12345678-...)
# üìä Importing 55 items from golden dataset...
# ‚úÖ Items imported: 55/55
# ‚úÖ Analyses imported: 55/55
# ‚úÖ Embeddings imported: 55/55
# üéâ Database seeded successfully!
```

#### 4. scripts/aws/cognito/create-user.sh

**Purpose**: Create test user and obtain JWT token

```bash
#!/bin/bash
# Features:
# - Create user in Cognito
# - Auto-confirm (admin operation)
# - Fetch JWT token
# - Save token to .test-user-token
# - Save user_id (sub) to .test-user-id

# Usage:
./scripts/aws/cognito/create-user.sh \
  --email testuser@example.com \
  --password Test123!

# Output:
# ‚úÖ User created: testuser@example.com
# üë§ User ID (sub): 12345678-1234-1234-1234-123456789012
# üîë JWT Token: eyJraWQiOiJ...
#
# Credentials saved:
# - .test-user-token (for API requests)
# - .test-user-id (for database operations)
#
# To use in API calls:
# export TOKEN=$(cat .test-user-token)
# curl -H "Authorization: Bearer $TOKEN" $API_URL/items
```

#### 5. scripts/aws/test/test-infrastructure.sh

**Purpose**: Comprehensive 10-step infrastructure validation

```bash
#!/bin/bash
# Tests:
# 1. RDS connection (psql, create/drop test table, pgvector)
# 2. Parameter Store (create/read/delete test parameter)
# 3. Cognito (create user, get JWT, verify sub claim)
# 4. S3 (upload/download file, check EventBridge config)
# 5. Lambda invoke (basic hello world)
# 6. Lambda‚ÜíRDS connection
# 7. Lambda‚ÜíParameter Store access
# 8. API Gateway routing
# 9. EventBridge‚ÜíLambda trigger (S3 upload)
# 10. End-to-end authenticated API call

# Usage:
./scripts/aws/test/test-infrastructure.sh

# Output:
# üîç Testing AWS Infrastructure...
#
# ‚úÖ 1/10 RDS connection test passed
# ‚úÖ 2/10 Parameter Store test passed
# ‚úÖ 3/10 Cognito JWT test passed
# ‚úÖ 4/10 S3 operations test passed
# ‚úÖ 5/10 Lambda invoke test passed
# ‚úÖ 6/10 Lambda‚ÜíRDS test passed
# ‚úÖ 7/10 Lambda‚ÜíParameter Store test passed
# ‚úÖ 8/10 API Gateway test passed
# ‚úÖ 9/10 EventBridge workflow test passed
# ‚úÖ 10/10 Authenticated API test passed
#
# üéâ All infrastructure tests passed!
#
# Test report: reports/infra-test-2025-12-22-143022.md
```

#### 6. scripts/migrate/sqlite-to-postgres.py

**Purpose**: Migrate SQLite database to PostgreSQL

```python
# Workflow:
# 1. Export SQLite to JSON (use existing export_db.py)
# 2. Transform schema:
#    - Add user_id column (from Cognito test user)
#    - Convert TEXT‚ÜíJSONB for raw_response, embedding_source
#    - Convert sqlite-vec embeddings to pgvector format
#    - Add user_id indexes
# 3. Import to PostgreSQL
# 4. Validate counts match

# Usage:
python scripts/migrate/sqlite-to-postgres.py \
  --sqlite-db data/collections.db \
  --postgres-url $DATABASE_URL \
  --cognito-user-id $(cat .test-user-id) \
  --dataset golden  # or 'full'

# Output:
# üìä Exporting SQLite database...
# ‚úÖ Exported 55 items, 55 analyses, 55 embeddings
#
# üîÑ Transforming schema...
# ‚úÖ Added user_id columns
# ‚úÖ Converted TEXT‚ÜíJSONB
# ‚úÖ Converted embeddings to pgvector format
#
# üì§ Importing to PostgreSQL...
# ‚úÖ Items: 55/55
# ‚úÖ Analyses: 55/55
# ‚úÖ Embeddings: 55/55
#
# ‚úÖ Migration validation passed
# üéâ Migration complete!
```

#### 7. scripts/migrate/images-to-s3.py

**Purpose**: Upload local images to S3 with user isolation

```python
# Workflow:
# For each image in data/images/:
#   - Upload to s3://bucket/{user_id}/images/{filename}
#   - Update database file_path column
#   - Optional: Generate thumbnail, upload to s3://.../thumbnails/

# Usage:
python scripts/migrate/images-to-s3.py \
  --images-dir data/images \
  --bucket-name $BUCKET_NAME \
  --user-id $(cat .test-user-id) \
  --generate-thumbnails  # Optional

# Output:
# üìÅ Found 55 images in data/images/
#
# üì§ Uploading images...
# [1/55] ‚úÖ uploaded: 12345678-uuid.jpg ‚Üí s3://bucket/user-id/images/
# [2/55] ‚úÖ uploaded: 87654321-uuid.png ‚Üí s3://bucket/user-id/images/
# ...
# [55/55] ‚úÖ uploaded: abcdef12-uuid.jpg ‚Üí s3://bucket/user-id/images/
#
# üñºÔ∏è  Generating thumbnails...
# [55/55] ‚úÖ thumbnail created
#
# üóÑÔ∏è  Updating database file_path columns...
# ‚úÖ Updated 55 records
#
# üéâ Image migration complete!
```

### Extending Existing Scripts

#### Modify: scripts/evaluate_retrieval.py

**Changes to support AWS**:

```python
# 1. Add AWS API endpoint detection
if os.path.exists('.aws-outputs.json'):
    with open('.aws-outputs.json') as f:
        outputs = json.load(f)
        api_url = outputs.get('ApiUrl')
        if api_url:
            api_candidates.append(api_url)
            print(f"üîç Found AWS API: {api_url}")

# 2. Add Cognito JWT authentication
if os.path.exists('.test-user-token'):
    with open('.test-user-token') as f:
        token = f.read().strip()
        headers['Authorization'] = f'Bearer {token}'
        print(f"üîë Using JWT authentication")

# 3. Support remote image URLs (S3 pre-signed URLs)
# No changes needed - existing code handles URLs

# Usage remains the same:
python scripts/evaluate_retrieval.py

# Auto-detects:
# - Local API (http://localhost:8000)
# - AWS API (from .aws-outputs.json)
# - Adds JWT auth automatically if .test-user-token exists
```

**New Make target for comparison**:

```bash
# make eval-compare
# Runs evaluation against both local and AWS, compares metrics
#
# Output:
# üìä Running evaluation against local API...
# ‚úÖ Local evaluation complete: reports/eval_local_20251222_143022.md
#
# üìä Running evaluation against AWS API...
# ‚úÖ AWS evaluation complete: reports/eval_aws_20251222_143145.md
#
# üìà Comparison:
# Metric              Local    AWS      Diff
# ------------------------------------------
# Precision@10        0.82     0.81     -0.01
# Recall@10           0.75     0.74     -0.01
# MRR                 0.88     0.87     -0.01
# NDCG@10             0.85     0.84     -0.01
# Avg Response Time   120ms    245ms    +125ms
#
# ‚ö†Ô∏è  AWS is slightly slower (Lambda cold starts)
# ‚úÖ Retrieval quality is comparable
```

### Configuration Files

#### .aws-outputs.json (Auto-generated by deploy.sh)

```json
{
  "RdsEndpoint": "collections-dev.xxxxx.us-east-1.rds.amazonaws.com",
  "ApiUrl": "https://xxxxxxxxxx.execute-api.us-east-1.amazonaws.com",
  "CognitoUserPoolId": "us-east-1_XXXXXXXXX",
  "CognitoClientId": "xxxxxxxxxxxxxxxxxxxxxxxxxx",
  "BucketName": "collections-dev-images-xxxxx",
  "DatabaseUrl": "postgresql://user:pass@host:5432/collections"
}
```

#### .gitignore additions

```gitignore
# AWS outputs and credentials
.aws-outputs.json
.test-user-token
.test-user-id

# CDK
infrastructure/cdk.out/
infrastructure/.cdk.staging/

# Environment configs with secrets
config/.env.aws
```

### Typical Development Workflows

#### Initial Setup (First Time)

```bash
# 1. Deploy infrastructure
make infra-deploy

# 2. Populate secrets from .env
make secrets-populate

# 3. Seed golden dataset
make db-seed-golden

# 4. Create test user
make cognito-create-user

# 5. Migrate images to S3
make migrate-images

# 6. Test everything
make test-all

# 7. Run evaluation
make eval-aws
```

**Time**: ~15-20 minutes for complete setup

#### Daily Development (Code Changes)

```bash
# 1. Make changes to Lambda code
vim app/main.py

# 2. Quick deploy (just Lambda, no CDK)
make lambda-deploy-api

# 3. Test API
make test-api

# 4. Check logs if needed
make lambda-logs FUNC=api

# 5. Run evaluation
make eval-aws
```

**Time**: ~2-3 minutes for code update and test

#### Testing After Infrastructure Changes

```bash
# 1. See what changed
make infra-diff

# 2. Deploy changes
make infra-deploy

# 3. Validate infrastructure
make test-infra

# 4. Test API
make test-api
```

**Time**: ~5-10 minutes

#### Comparing Local vs AWS

```bash
# Run evaluation on both and compare metrics
make eval-compare

# Shows side-by-side comparison of:
# - Retrieval quality metrics (Precision, Recall, MRR, NDCG)
# - Response times
# - Any differences in search results
```

#### Clean Slate Reset

```bash
# 1. Destroy all AWS resources
make infra-destroy

# 2. Clean local artifacts
make clean

# 3. Start fresh
make infra-deploy
make db-seed-golden
```

### Success Metrics

Good tooling enables:
1. ‚úÖ Deploy full stack in < 10 minutes
2. ‚úÖ Deploy Lambda code update in < 2 minutes
3. ‚úÖ Seed test data in < 5 minutes
4. ‚úÖ Run full test suite in < 3 minutes
5. ‚úÖ Zero manual AWS Console clicks for common tasks
6. ‚úÖ Easy comparison between local and AWS performance

### Files to Create

**Priority 1 - Essential** (Week 1):
1. `Makefile` - Primary developer interface
2. `scripts/aws/infra/deploy.sh` - CDK deployment wrapper
3. `scripts/aws/secrets/populate.sh` - Secrets management
4. `scripts/aws/db/seed.sh` - Database seeding
5. `scripts/aws/cognito/create-user.sh` - User creation
6. `scripts/aws/test/test-infrastructure.sh` - 10-step validation

**Priority 2 - High Value** (Week 2):
7. `scripts/migrate/sqlite-to-postgres.py` - Data migration
8. `scripts/migrate/images-to-s3.py` - Image migration
9. `scripts/aws/lambda/deploy-api.sh` - Fast Lambda updates
10. `scripts/aws/test/test-api.sh` - API testing

**Priority 3 - Quality of Life** (Week 3):
11. `scripts/aws/lambda/logs.sh` - CloudWatch log tailing
12. Extend `scripts/evaluate_retrieval.py` for AWS
13. Add comparison tooling (`make eval-compare`)

**Total**: ~1500 lines of new automation code

---

## Next Steps

1. ‚úÖ Review this plan
2. ‚¨ú Set up AWS account (if needed)
3. ‚¨ú Install AWS CLI and CDK
4. ‚¨ú Start Phase 1: Infrastructure setup
5. ‚¨ú Populate Parameter Store with existing API keys
6. ‚¨ú Deploy CDK stack
7. ‚¨ú Begin database migration

---

## Questions to Address

Before starting implementation:

- [ ] AWS account ready? (with admin access)
- [ ] Domain name for API? (or use AWS-generated URL)
- [ ] Region preference? (us-east-1 recommended for cost)
- [ ] Backup strategy? (RDS automated backups enabled by default)
- [ ] CI/CD needed? (GitHub Actions ‚Üí CDK deploy)
- [ ] Monitoring requirements beyond CloudWatch?

---

## References

- AWS CDK Documentation: https://docs.aws.amazon.com/cdk/
- pgvector Extension: https://github.com/pgvector/pgvector
- Cognito JWT Validation: https://docs.aws.amazon.com/cognito/latest/developerguide/amazon-cognito-user-pools-using-tokens-verifying-a-jwt.html
- Mangum (ASGI adapter): https://github.com/jordaneremieff/mangum
- Lambda Container Images: https://docs.aws.amazon.com/lambda/latest/dg/images-create.html

---

**Document Version**: 2.0 (Simplified)
**Last Updated**: 2025-12-22
**Author**: Migration Planning Session

**Changelog**:
- v2.0: Simplified architecture - removed VPC, RLS, and users table
- v1.0: Initial plan with VPC and Row-Level Security
